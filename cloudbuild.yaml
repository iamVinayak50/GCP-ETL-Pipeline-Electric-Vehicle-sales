steps:
  # Upload DAG to Cloud Composer
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Upload DAG'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "ðŸ“¤ Uploading DAG to Composer bucket..."
        gsutil cp dags/ev_pipeline_dag.py gs://us-central1-etl-airflow-33be8bc7-bucket/dags/

  # Upload PySpark ETL script
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Upload PySpark Script'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "ðŸ“¤ Uploading PySpark script..."
        gsutil cp scripts/etl_ev.py gs://ev-market-data/scripts/etl_ev.py

timeout: 600s
